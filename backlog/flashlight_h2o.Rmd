---
title: "Using flashlight with h2o"
author: "Michael Mayer"
date: "`r Sys.Date()`"
bibliography: "biblio.bib"
link-citations: true
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{flashlight_h2o}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 6
)
```

```{r setup}
library(dplyr)
library(MetricsWeighted)
library(flashlight)
library(caret)
library(h2o)

h2o.init()
h2o.no_progress()
```

## Introduction

This vignette shows how to use `flashlight` for interpretation of models trained with the `h2o` package. 

Currently, the **use of `flashlight` with `h2o` is limited** by the fact that data modifications (e.g. for permutation importance) are done in R. This means that data sets are passed multiple times from R to h2o. This is too much overhead for **data sets above 10'000 rows**, say. In the future, this might be solved by moving data modifications to the h2o backend.

## Training a linear regression and a random forest on the `cars` data set

The `caret` package contains some wonderful data sets for playing, e.g. the cars data set.


```{r}
data(cars)
str(cars)
```

We then use the data to fit two types of regression to predict log(Price) by log(Mileage) and the other covariables. Car mades are already dummy coded. We will revert this in order to simplify the explainer process. Additionally, we represent some 0-1 dummies by nice, meaningful factors.

```{r}
undo_dummies <- function(df, cols) {
  factor(data.matrix(df[, cols]) %*% seq_along(cols), labels = cols)
}

no_yes <- function(x) {
  factor(x, 0:1, c("no", "yes"))
}

# Prepare data
cars <- cars %>% 
  mutate(Price = log(Price),
         Mileage = log(Mileage),
         Made = undo_dummies(., c("Buick", "Cadillac", "Chevy", "Pontiac", "Saab", "Saturn"))) %>% 
  mutate_at(c("Cruise", "Sound", "Leather"), no_yes)

# Response and covariables
y <- "Price"
x <- c("Cylinder", "Doors", "Cruise", "Sound", "Leather", "Mileage", "Made")

# Data split
set.seed(1)
idx <- c(createDataPartition(cars[[y]], p = 0.7, list = FALSE))
tr <- cars[idx, c(y, x)]
te <- cars[-idx, c(y, x)]

# Fit the models
fit_lm <- h2o.glm(x, y, as.h2o(tr))
fit_rf <- h2o.randomForest(x, y, as.h2o(tr))
```

## flashlights

Then we collect all infos to build (multi-)flashlights, the core objects for explaining and comparing the models.

```{r}
pred_fun <- function(mod, X) as.vector(unlist(h2o.predict(mod, as.h2o(X))))
fl_lm <- flashlight(model = fit_lm, label = "lm", predict_function = pred_fun)
fl_rf <- flashlight(model = fit_rf, label = "rf", predict_function = pred_fun)

fls <- multiflashlight(list(fl_lm, fl_rf), y = y, data = te, 
                       metrics = list(RMSE = rmse, `R-Squared` = r_squared))
```

## Explaining the models

Let us go through a selection of explainability tools.

### Performance

The models perform essentially similar.

```{r}
light_performance(fls) %>% 
  plot(fill = "darkred")
```

### Importance

Let's study permutation importance regarding RMSE metric.

```{r}
imp <- light_importance(fls) 
plot(imp, fill = "darkred")
```


### Effects

Now, let's look at a couple of ways to visualize effects.

```{r}
# Individual conditional expectations (ICE). Using a seed guarantees the same observations across models
light_ice(fls, v = "Cylinder", n_max = 100, seed = 54) %>% 
  plot(alpha = 0.1)

# Partial dependence profiles
light_profile(fls, v = "Cylinder") %>% 
  plot()

light_profile(fls, v = "Cylinder", by = "Leather") %>% 
  plot()

# Accumulated local effects
light_profile(fls, v = "Cylinder", type = "ale") %>% 
  plot()

# M-Plots
light_profile(fls, v = "Mileage", type = "predicted") %>% 
  plot()

# Response profiles, prediction profiles, partial dependence in one
eff <- light_effects(fls, v = "Cylinder") 
eff %>% 
  plot() %>% 
  plot_counts(eff, alpha = 0.3)
```

### Interaction strength

How strong are the pairwise interactions among the three most important predictors? Surprise, surprice: for the linear regression, there are none!

```{r}
light_interaction(fls, v = most_important(imp, 3), pairwise = TRUE, 
                           n_max = 30, seed = 63) %>% 
  plot(fill = "darkred")
```  

### Global surrogate

```{r}
light_global_surrogate(fls) %>% 
  plot()
```
## Close the session

```{r}
h2o.shutdown()
```
